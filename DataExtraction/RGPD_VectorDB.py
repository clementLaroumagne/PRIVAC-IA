import pandas as pd
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import PromptTemplate
from typing import AsyncIterable
import json
import uuid
import asyncio
from openai import AsyncOpenAI
import os
import shutil
from dotenv import load_dotenv

load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')
client = AsyncOpenAI(api_key=openai_api_key)
RGPD_FILE = "./Extractions/REGLEMENT_GENERAL_PROTECTION_DONNEES.csv"
ARTICLES_FILE = "./Extractions/ARTICLES_RGPD.csv"
SANCTIONS_FILE = "./Extractions/ALL_SANCTIONS_CNIL.csv"

VECTOR_DB_PATH = "../rgpd_embeddings_db"

prompt_template = PromptTemplate(
    template="""En utilisant le contexte suivant, r√©ponds √† la question.
    
    Contexte: {context}
    
    Question: {query}
    
    R√©ponse:""",
    input_variables=["context", "query"]
)

llm = ChatOpenAI(model="gpt-4o-mini", openai_api_key=openai_api_key)
chain = prompt_template | llm | RunnablePassthrough()

def load_and_process_csv(file_path, text_columns, metadata_columns, source_name):
    df = pd.read_csv(file_path, sep=',', encoding='utf-8')

    for col in text_columns + metadata_columns:
        if col not in df.columns:
            df[col] = "" 
    
    df['texte_complet'] = df[text_columns].apply(lambda x: ' - '.join(x.astype(str)), axis=1)
    
    df['source'] = source_name
    
    return df[['texte_complet'] + metadata_columns + ['source']]

def create_vector_database():
    print("üóëÔ∏è Suppression compl√®te de la base...")
    if os.path.exists(VECTOR_DB_PATH):
        shutil.rmtree(VECTOR_DB_PATH) 

    print("üì• Chargement des fichiers...")
    df_rgpd = load_and_process_csv(RGPD_FILE, ['Numero alinea', 'Texte'], ['Numero alinea', 'Texte'], "RGPD")
    df_articles = load_and_process_csv(ARTICLES_FILE, ['Chapitre', 'Article', 'Alinea', 'Sous-Alinea'], ['Chapitre', 'Article'], "Articles RGPD")
    df_sanctions = load_and_process_csv(SANCTIONS_FILE, ['Date', 'Type_Organisme', 'Manquement', 'Sanction'], ['Date', 'Type_Organisme', 'Manquement', 'Sanction'], "Sanctions CNIL")

    datasets = [df_rgpd, df_articles, df_sanctions]
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)


    print("üÜï Cr√©ation d'une nouvelle base vectorielle...")
    vectorstore = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)

    print("üÜï Ajout des nouvelles donn√©es...")
    for df in datasets:
        documents = df['texte_complet'].tolist()
        metadatas = df.drop(columns=['texte_complet']).to_dict(orient='records')

        vectorstore.add_texts(texts=documents, metadatas=metadatas)

    print("‚úÖ Base de donn√©es vectorielle mise √† jour.")


async def query_vector_database_stream(query: str) -> AsyncIterable[str]:
    print("D√©marrage de la requ√™te...")
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
    
    print("Recherche dans la base vectorielle...")
    relevant_documents = vectorstore.similarity_search(query, k=5)
    context = "\n".join([doc.page_content for doc in relevant_documents])
    print(f"Contexte trouv√©: {context[:100]}...")
    
    message_id = f"msg-{uuid.uuid4().hex}"
    print(f"ID du message g√©n√©r√©: {message_id}")
    
    messages = [
        {"role": "system", "content": "Tu es un expert du RGPD qui r√©pond de mani√®re pr√©cise et concise. Tu as acc√®s √† une base de donn√©es de documents juridiques. Tu dois r√©pondre √† la question en ajoutant les articles cit√©s ainsi que leur num√©ro quand cela est possible."},
        {"role": "user", "content": f"En utilisant le contexte suivant, r√©ponds √† la question.\n\nContexte: {context}\n\nQuestion: {query}\n\nR√©ponse:"}
    ]

    print("D√©marrage du streaming avec OpenAI...")
    try:
        # Stream la r√©ponse de GPT-4
        stream = await client.chat.completions.create(
            model="gpt-4",
            messages=messages,
            stream=True
        )

        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                token = chunk.choices[0].delta.content
                print("Token re√ßu :", token)
                yield token
                
        print("Fin du streaming")

    except Exception as e:
        print(f"Erreur pendant le streaming: {e}")
        yield json.dumps({"error": str(e)}) + "\n"

async def test_query():
    query = "de quoi parle l'article 5 ?"
    print("Lancement du test...")

    try:
        async for chunk in query_vector_database_stream(query):
            print("Chunk re√ßu :", chunk.strip())
    except Exception as e:
        print(f"Erreur pendant le test : {e}")

if __name__ == "__main__":
    print("D√©marrage du script...")
    # create_vector_database(csv_file_path)  # D√©commenter pour cr√©er la base
    asyncio.run(test_query())